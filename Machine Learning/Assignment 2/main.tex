\documentclass[a4paper]{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}

\makeatletter
\newcommand{\leqnomode}{\tagsleft@true}
\newcommand{\reqnomode}{\tagsleft@false}
\makeatother

\input{head}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Julien Fer \hfill\\   
10649441\hfill\\
julien.fer88@gmail.com
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Homework Assignment 2\\ 
\normalsize 
Machine Learning 1, 19/20\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------

\section{\underline{MAP solution with correlated responses}}
\subsection{Question 1}
\textbf{a)} \textit{Write down the likelihood $p(\mathcal{D} \mid \bm{\theta})$ in vector/matrix form, i.e. in terms of $\textbf{t}, \bm{\Psi}, \textbf{w}$ and $\bm{\Omega}$. Note that the distribution can not be factored into independent multiplicands in this basis.}
\begin{flalign}
p(\textbf{D} \mid \bm{\theta}) &= p(\textbf{t} \mid \bm{\Psi}, \textbf{w}, \bm{\Omega}) \notag \\
& = \mathcal{N}(\textbf{t} \mid \bm{\Psi}\textbf{w}, \bm{\Omega}) \notag \\ 
& = \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\bm{\Omega})}} \exp\left(-\frac{1}{2}(\textbf{t} - \bm{\Psi}\textbf{w})^{T}\bm{\Omega}^{-1}(\textbf{t} - \bm{\Psi}\textbf{w})\right) \label{eq:1}
&&
\end{flalign}

\bigskip

\textbf{b)} \textit{Write the likelihood in terms of a Gaussian distribution with a diagonal covariance matrix by changing the basis of the space in which the targets are expressed. Specifically, express the covariance matrix in its eigenbasis, i.e. write it as $\bm{\Omega} = \textbf{A}^{T}\textbf{DA}$ with $\textbf{D} \coloneqq$ diag($d_{1}$, $\dots$, $d_{N}$) being a diagonal matrix containing the eigenvalues of $\bm{\Omega}$ and $\textbf{A}^{T} = \textbf{A}^{-1}$ being an \underline{orthogonal} change of basis. This is possible in general since covariance matrices are symmetric.}
\newline
\newline
Let $\bm{\Omega} \coloneqq \textbf{A}^{T}\textbf{DA}$. From \eqref{eq:1} we see that it is necessary to determine the determinant and the inverse of the `new` defined matrix $\bm{\Omega}$:
\begin{flalign}
\text{det}(\bm{\Omega}) & = \text{det}(\textbf{A}^{T}\textbf{DA}) \notag \\
& = \text{det}(\textbf{A}^{-1}\textbf{DA}) ~~~~\text{(as \textbf{A} is orthogonal)} \notag \\
& = \text{det}(\textbf{A}^{-1}) \text{det}(\textbf{D}) \text{det}(\textbf{A}) = \text{det}(\textbf{A})^{-1}\text{det}(\textbf{D}) \text{det}(\textbf{A}) \notag \\
&= \text{det}(\textbf{D}) \label{eq:2} \\
\bm{\Omega}^{-1} &= (\textbf{A}^{T}\textbf{DA})^{-1} = (\textbf{A}^{-1}) (\textbf{D}^{-1}) (\textbf{A}^{T})^{-1} \notag \\
&= \textbf{A}^{T} \textbf{D}^{-1} \textbf{A}~~~~\text{(as \textbf{A} is orthogonal)} \label{eq:3}
&&
\end{flalign}
Substituting \eqref{eq:2} and \eqref{eq:3} in \eqref{eq:1} gives the following likelihood:
\begin{flalign}
p(\mathcal{D} \mid \bm{\theta}) &= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\bm{\Omega})}} \exp\left(-\frac{1}{2}\bigl[(\textbf{t} - \bm{\Psi}\textbf{w})^{T}\bm{\Omega}^{-1}(\textbf{t} - \bm{\Psi}\textbf{w})\right) \notag \\
&=\frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\textbf{t} - \bm{\Psi}\textbf{w})^{T}\textbf{A}^{T}\textbf{D}^{-1}\textbf{A}(\textbf{t} - \bm{\Psi}\textbf{w})\right) \notag \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\textbf{t}^{T} \textbf{A}^{T} \textbf{D}^{-1}\textbf{At} - \textbf{t}^{T}\textbf{A}^{T} \textbf{D}^{-1}\textbf{A} \bm{\Psi} \textbf{w} - \textbf{w}^{T} \bm{\Psi}^{T} \textbf{A}^{T} \textbf{D}^{-1}\textbf{At} + \textbf{w}^{T} \bm{\Psi}^{T} \textbf{A}^{T} \textbf{D}^{-1}\textbf{A} \bm{\Psi} \textbf{w})\right) \notag \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\bm{\tau}^{T} \textbf{D}^{-1} \bm{\tau} - \bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T}\textbf{D}^{-1} \bm{\tau} + \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w})\right) \notag \\
&=\frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\bm{\tau}^{T} \textbf{D}^{-1} \bm{\tau} - 2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} + \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w})\right) ~~~~\text{(as $\textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\tau}$ is scalar/symmetrical)} \notag \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\bm{\tau} - \bm{\Phi} \textbf{w})^{T} \textbf{D}^{-1} (\bm{\tau} - \bm{\Phi} \textbf{w})\right) \notag \\ &= \mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D}) \label{eq:4}
&&
\end{flalign}
with $\bm{\tau} \coloneqq$ \textbf{At} and $\bm{\Phi} \coloneqq$ \textbf{A}$\bm{\Psi}$

\bigskip

\textbf{c)} \textit{Factorize the distribution, into a product of univariate Gaussians. Write $\phi_{i}$ for the $i$-th row of the matrix $\Phi$ Hint: The determinant of a diagonal matrix can be expressed as the product of its elements.}:
\newline
\newline
We can define the determinant of \eqref{eq:2} as det(\textbf{D}) = $\prod_{i = 1}^{N} d_{ii}$. Substiting this in \eqref{eq:4} yields:
\begin{flalign}
p(\mathcal{D} \mid \bm{\theta}) &= \frac{1}{\sqrt{(2 \pi)^{N} \prod_{i = i}^{N} d_{ii}}} \exp \left(-\frac{1}{2} \sum_{i = 1}^{N} (\tau_{i} - \bm{\phi}_{i}\textbf{w})^{2} \frac{1}{d_{ii}} \right) \notag \\
&= \prod_{i = i}^{N} \frac{1}{\sqrt{2 \pi d_{ii}}} \exp\left(\frac{1}{2 d_{ii}} \left(\tau_{i} - \bm{\phi}_{i} \textbf{w} \right)^{2} \right) \notag \\
&= \prod_{i = i}^{N} \mathcal{N}(\tau_{i} \mid \bm{\phi}_{i} \textbf{w},~ d_{ii}) \label{eq:5}
&&
\end{flalign}

\bigskip

\textbf{d)} \textit{Write down the explicit form of the prior $p(\textbf{w})$, i.e. use the expression for a multivariate Gaussian distribution with the correct mean and covariance. Compute the logarithm of the prior $\ln p(\textbf{w})$.}
\begin{flalign}
p(\textbf{w}) &= \mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I}) \notag \\
&= \frac{1}{\sqrt{(2\pi)^{M} \text{det}(\alpha \textbf{I})}} \exp\left(-\frac{1}{2} (\textbf{w} - \bm{0})^{T}(\alpha \textbf{I})^{-1}(\textbf{w} - \bm{0})  \right) \notag \\
&= \frac{1}{\sqrt{(2 \pi \alpha)^{M}}} \exp\left(-\frac{\alpha}{2} \textbf{w}^{T} \textbf{w} \right) \label{eq:6}
&&
\end{flalign}
Subsequentely, we take the natural logarithm of \eqref{eq:6} to obtain the logarithm of the prior:
\begin{flalign}
\ln p(\textbf{w}) &= -\frac{M}{2} \ln(2\pi\alpha) - \frac{\alpha}{2}\textbf{w}^{T}\textbf{w} \notag \\
&= -\frac{\alpha}{2}\textbf{w}^{T}\textbf{w} + C \label{eq:7}
&&
\end{flalign}

\bigskip

\textbf{e)} \textit{Write down an expression for the posterior $p(\textbf{w} \mid \textbf{D})$ over \textbf{w} by applying Bayes rule. You do not need to write out the explicit form of the Gaussian distributions, instead use the form $\mathcal{N}(a \mid b, c^{2})$ with appropriate means $b$ and variances $c^{2}$. Show that the evidence will require an integral, which you do not need to solve analytically! However, you need to replace it with a probability distribution like $p(a \mid b, c^{2})$ with thecorrect corresponding variables and conditioning variables. (Note that $p(a \mid b, c^{2})$ is just an example, there might be more or less than 2 conditioning variables.)}
\newline
\newline
The Bayes rule is given by:
\begin{flalign*}
\overbrace{p(\textbf{w} \mid \mathcal{D})}^{posterior} &= \frac{\overbrace{p(\mathcal{D} \mid \textbf{w})}^{likelihood} \overbrace{p(\textbf{w})}^{prior}}{\underbrace{p(\mathcal{D})}_{evidence}}
&&
\end{flalign*}
From \eqref{eq:5} we retrieve the \textit{likelihood} and from \eqref{eq:6} the \textit{prior}. By applying the product rule with \eqref{eq:5} and \eqref{eq:6} and integrating over the entire domain of \textbf{w} we retrieve the \textit{evidence}.
\begin{flalign}
p(\textbf{w} \mid \mathcal{D}) &= \frac{\prod_{i = i}^{N} \mathcal{N}(\tau_{i} \mid \bm{\phi}_{i} \textbf{w},~ d_{ii}) ~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})}{\int \prod_{i = i}^{N} \mathcal{N}(\tau_{i} \mid \bm{\phi}_{i} \textbf{w},~ d_{ii}) ~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I}) \mathrm{d}\textbf{w}} \notag \\
&= \frac{\mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D})~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})}{\int  \mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D})~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})} \notag \\
&= \frac{\mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D})~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})}{p(\bm{\tau} \mid \bm{\Phi}, \textbf{D}, \alpha)} \label{eq:8}
&&
\end{flalign}

\bigskip

\textbf{f)} \textit{Compute the log-posterior for both the matrix form of the likelihood as derived in 1.2 and the factorized component form of the likelihood as derived in 1.3. Collect all terms which are independent of \textbf{w} into a constant $C$. Which parts of the previous expression do not depend on \textbf{w}? Why is finding the MAP much simpler than finding the full posterior distribution?}
\newline
\newline
If we apply Bayes rule as in exercise \textbf{e)} and take the logarithm one can find the log-posterior:
\begin{flalign}
\ln p(\textbf{w} \mid \mathcal{D}) &= \ln p(\mathcal{D} \mid \textbf{w}) + \ln p(\textbf{w}) - \ln p(\mathcal{D}) \label{eq:9}
&&
\end{flalign}
First we apply


\end{document}
