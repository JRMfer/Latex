\documentclass[a4paper]{article}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}

\makeatletter
\newcommand{\leqnomode}{\tagsleft@true}
\newcommand{\reqnomode}{\tagsleft@false}
\makeatother

\input{head}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
Julien Fer \hfill\\   
10649441\hfill\\
julien.fer88@gmail.com
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Homework Assignment 2\\ 
\normalsize 
Machine Learning 1, 19/20\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------

\section{\underline{MAP solution with correlated responses}}
\subsection{Question 1}
\textbf{a)} \textit{Write down the likelihood $p(\mathcal{D} \mid \bm{\theta})$ in vector/matrix form, i.e. in terms of $\textbf{t}, \bm{\Psi}, \textbf{w}$ and $\bm{\Omega}$. Note that the distribution can not be factored into independent multiplicands in this basis.}
\begin{flalign}
p(\textbf{D} \mid \bm{\theta}) &= p(\textbf{t} \mid \bm{\Psi}, \textbf{w}, \bm{\Omega}) \notag \\
& = \mathcal{N}(\textbf{t} \mid \bm{\Psi}\textbf{w}, \bm{\Omega}) \notag \\ 
& = \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\bm{\Omega})}} \exp\left(-\frac{1}{2}(\textbf{t} - \bm{\Psi}\textbf{w})^{T}\bm{\Omega}^{-1}(\textbf{t} - \bm{\Psi}\textbf{w})\right) \label{eq:1}
&&
\end{flalign}

\bigskip

\textbf{b)} \textit{Write the likelihood in terms of a Gaussian distribution with a diagonal covariance matrix by changing the basis of the space in which the targets are expressed. Specifically, express the covariance matrix in its eigenbasis, i.e. write it as $\bm{\Omega} = \textbf{A}^{T}\textbf{DA}$ with $\textbf{D} \coloneqq$ diag($d_{1}$, $\dots$, $d_{N}$) being a diagonal matrix containing the eigenvalues of $\bm{\Omega}$ and $\textbf{A}^{T} = \textbf{A}^{-1}$ being an \underline{orthogonal} change of basis. This is possible in general since covariance matrices are symmetric.}
\newline
\newline
Let $\bm{\Omega} \coloneqq \textbf{A}^{T}\textbf{DA}$. From \eqref{eq:1} we see that it is necessary to determine the determinant and the inverse of the `new` defined matrix $\bm{\Omega}$:
\begin{flalign}
\text{det}(\bm{\Omega}) & = \text{det}(\textbf{A}^{T}\textbf{DA}) \notag \\
& = \text{det}(\textbf{A}^{-1}\textbf{DA}) ~~~~\text{(as \textbf{A} is orthogonal)} \notag \\
& = \text{det}(\textbf{A}^{-1}) \text{det}(\textbf{D}) \text{det}(\textbf{A}) = \text{det}(\textbf{A})^{-1}\text{det}(\textbf{D}) \text{det}(\textbf{A}) \notag \\
&= \text{det}(\textbf{D}) \label{eq:2} \\
\bm{\Omega}^{-1} &= (\textbf{A}^{T}\textbf{DA})^{-1} = (\textbf{A}^{-1}) (\textbf{D}^{-1}) (\textbf{A}^{T})^{-1} \notag \\
&= \textbf{A}^{T} \textbf{D}^{-1} \textbf{A}~~~~\text{(as \textbf{A} is orthogonal)} \label{eq:3}
&&
\end{flalign}
Substituting \eqref{eq:2} and \eqref{eq:3} in \eqref{eq:1} gives the following likelihood:
\begin{flalign}
p(\mathcal{D} \mid \bm{\theta}) &= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\bm{\Omega})}} \exp\left(-\frac{1}{2}(\textbf{t} - \bm{\Psi}\textbf{w})^{T}\bm{\Omega}^{-1}(\textbf{t} - \bm{\Psi}\textbf{w})\right) \notag \\
&=\frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\textbf{t} - \bm{\Psi}\textbf{w})^{T}\textbf{A}^{T}\textbf{D}^{-1}\textbf{A}(\textbf{t} - \bm{\Psi}\textbf{w})\right) \notag \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\textbf{t}^{T} \textbf{A}^{T} \textbf{D}^{-1}\textbf{At} - \textbf{t}^{T}\textbf{A}^{T} \textbf{D}^{-1}\textbf{A} \bm{\Psi} \textbf{w} - \textbf{w}^{T} \bm{\Psi}^{T} \textbf{A}^{T} \textbf{D}^{-1}\textbf{At} + \textbf{w}^{T} \bm{\Psi}^{T} \textbf{A}^{T} \textbf{D}^{-1}\textbf{A} \bm{\Psi} \textbf{w})\right) \notag \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\bm{\tau}^{T} \textbf{D}^{-1} \bm{\tau} - \bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T}\textbf{D}^{-1} \bm{\tau} + \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w})\right) \notag \\
&=\frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\bm{\tau}^{T} \textbf{D}^{-1} \bm{\tau} - 2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} + \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w})\right) ~~~~\text{(as $\textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\tau}$ is scalar/symmetrical)} \notag \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \text{det}(\textbf{D})}} \exp\left(-\frac{1}{2}(\bm{\tau} - \bm{\Phi} \textbf{w})^{T} \textbf{D}^{-1} (\bm{\tau} - \bm{\Phi} \textbf{w})\right) \notag \\ &= \mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D}) \label{eq:4}
&&
\end{flalign}
with $\bm{\tau} \coloneqq$ \textbf{At} and $\bm{\Phi} \coloneqq$ \textbf{A}$\bm{\Psi}$

\bigskip

\textbf{c)} \textit{Factorize the distribution, into a product of univariate Gaussians. Write $\phi_{i}$ for the $i$-th row of the matrix $\Phi$ Hint: The determinant of a diagonal matrix can be expressed as the product of its elements.}:
\newline
\newline
We can define the determinant of \eqref{eq:2} as det(\textbf{D}) = $\prod_{i = 1}^{N} d_{i}$. Substiting this in \eqref{eq:4} yields:
\begin{flalign}
p(\mathcal{D} \mid \bm{\theta}) &= \frac{1}{\sqrt{(2 \pi)^{N} \prod_{i = i}^{N} d_{i}}} \exp \left(-\frac{1}{2} \sum_{i = 1}^{N} (\tau_{i} - \bm{\phi}_{i}\textbf{w})^{2} \frac{1}{d_{i}} \right) \notag \\
&= \prod_{i = i}^{N} \frac{1}{\sqrt{2 \pi d_{i}}} \exp\left(-\frac{1}{2 d_{i}} \left(\tau_{i} - \bm{\phi}_{i} \textbf{w} \right)^{2} \right) \notag \\
&= \prod_{i = i}^{N} \mathcal{N}(\tau_{i} \mid \bm{\phi}_{i} \textbf{w},~ d_{i}) \label{eq:5}
&&
\end{flalign}

\bigskip

\textbf{d)} \textit{Write down the explicit form of the prior $p(\textbf{w})$, i.e. use the expression for a multivariate Gaussian distribution with the correct mean and covariance. Compute the logarithm of the prior $\ln p(\textbf{w})$.}
\begin{flalign}
p(\textbf{w}) &= \mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I}) \notag \\
&= \frac{1}{\sqrt{(2\pi)^{M} \text{det}(\alpha \textbf{I})}} \exp\left(-\frac{1}{2} (\textbf{w} - \bm{0})^{T}(\alpha \textbf{I})^{-1}(\textbf{w} - \bm{0})  \right) \notag \\
&= \frac{1}{\sqrt{(2 \pi \alpha)^{M}}} \exp\left(-\frac{1}{2\alpha} \textbf{w}^{T} \textbf{w} \right) \label{eq:6}
&&
\end{flalign}
Subsequentely, we take the natural logarithm of \eqref{eq:6} to obtain the logarithm of the prior:
\begin{flalign}
\ln p(\textbf{w}) &= -\frac{M}{2} \ln(2\pi\alpha) - \frac{1}{2\alpha}\textbf{w}^{T}\textbf{w} \notag \\
&= -\frac{1}{2\alpha}\textbf{w}^{T}\textbf{w} + C \label{eq:7}
&&
\end{flalign}

\bigskip

\textbf{e)} \textit{Write down an expression for the posterior $p(\textbf{w} \mid \textbf{D})$ over \textbf{w} by applying Bayes rule. You do not need to write out the explicit form of the Gaussian distributions, instead use the form $\mathcal{N}(a \mid b, c^{2})$ with appropriate means $b$ and variances $c^{2}$. Show that the evidence will require an integral, which you do not need to solve analytically! However, you need to replace it with a probability distribution like $p(a \mid b, c^{2})$ with thecorrect corresponding variables and conditioning variables. (Note that $p(a \mid b, c^{2})$ is just an example, there might be more or less than 2 conditioning variables.)}
\newline
\newline
The Bayes rule is given by:
\begin{flalign*}
\overbrace{p(\textbf{w} \mid \mathcal{D})}^{posterior} &= \frac{\overbrace{p(\mathcal{D} \mid \textbf{w})}^{likelihood} \overbrace{p(\textbf{w})}^{prior}}{\underbrace{p(\mathcal{D})}_{evidence}}
&&
\end{flalign*}
From \eqref{eq:5} we retrieve the \textit{likelihood} and from \eqref{eq:6} the \textit{prior}. By applying the product rule with \eqref{eq:5} and \eqref{eq:6} and integrating over the entire domain of \textbf{w} we retrieve the \textit{evidence}.
\begin{flalign}
p(\textbf{w} \mid \mathcal{D}) &= \frac{\prod_{i = i}^{N} \mathcal{N}(\tau_{i} \mid \bm{\phi}_{i} \textbf{w},~ d_{ii}) ~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})}{\int \prod_{i = i}^{N} \mathcal{N}(\tau_{i} \mid \bm{\phi}_{i} \textbf{w},~ d_{ii}) ~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I}) \mathrm{d}\textbf{w}} \notag \\
&= \frac{\mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D})~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})}{\int  \mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D})~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})} \notag \\
&= \frac{\mathcal{N}(\bm{\tau} \mid \bm{\Phi} \textbf{w}, \textbf{D})~\mathcal{N}(\textbf{w} \mid \bm{0}, \alpha \textbf{I})}{p(\bm{\tau} \mid \bm{\Phi}, \textbf{D}, \alpha)} \label{eq:8}
&&
\end{flalign}

\bigskip

\textbf{f)} \textit{Compute the log-posterior for both the matrix form of the likelihood as derived in 1.2 and the factorized component form of the likelihood as derived in 1.3. Collect all terms which are independent of \textbf{w} into a constant $C$. Which parts of the previous expression do not depend on \textbf{w}? Why is finding the MAP much simpler than finding the full posterior distribution?}
\newline
\newline
If we apply Bayes rule as in exercise \textbf{e)} and take the logarithm one can find the log-posterior:
\begin{flalign}
\ln p(\textbf{w} \mid \mathcal{D}) &= \ln p(\mathcal{D} \mid \textbf{w}) + \ln p(\textbf{w}) - \ln p(\mathcal{D}) \label{eq:9}
&&
\end{flalign}
First we apply this for the matrix form. The likelihood and prior are represented by respectively \eqref{eq:4} and \eqref{eq:6}. First we determine the logarithm of these probability distributions:
\begin{flalign}
\ln p(\mathcal{D} \mid \textbf{w}) &= -\frac{N}{2} \ln \left(2\pi\right) - \frac{1}{2} \ln \left(\text{det}(\textbf{D}) \right) - \frac{1}{2} \left(\bm{\tau}^{T}\textbf{D}^{-1}\bm{\tau} - 2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} + \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} \right) \notag \\
&= \frac{1}{2} \left(2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \bm{\tau}^{T}\textbf{D}^{-1}\bm{\tau} - N \ln(2\pi) - \ln(\text{det}(\textbf{D})) \right) \notag \\
&= \frac{1}{2} \left(2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} + C_{1} \right) \label{eq:10}
&&
\end{flalign}
with $C_{1} = -\bm{\tau}^{T}\textbf{D}^{-1}\bm{\tau} - N \ln(2\pi) - \ln(\text{det}(\textbf{D}))$.
\newline
\newline
In \eqref{eq:7} we already found the constant value of the log-prior:
\begin{flalign}
\ln p(\textbf{w}) &= -\frac{M}{2} \ln(2\pi\alpha) - \frac{1}{2\alpha}\textbf{w}^{T}\textbf{w} \notag \\
&= -\frac{1}{2\alpha}\textbf{w}^{T}\textbf{w} + C_{2} \label{eq:11}
&&
\end{flalign}
with $C_{2} = -\frac{M}{2} \ln(2\pi\alpha)$ 
\newline
\newline
This leaves us with the log-evidence. As this does not depend on \textbf{w} we can set this equal to a constant:
\begin{flalign}
\ln p(\mathcal{D}) &= C_{3} \label{eq:12}
&&
\end{flalign}
Substituting \eqref{eq:10}, \eqref{eq:11} and \eqref{eq:12} in \eqref{eq:9} yields:
\begin{flalign}
\ln p(\textbf{w} \mid \mathcal{D}) &= \frac{1}{2} \left(2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} + C_{1} \right)  - \frac{1}{2\alpha}\textbf{w}^{T}\textbf{w} + C_{2} +  C_{3} \notag \\
&= \frac{1}{2} \left(2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} \right) -\frac{1}{2\alpha}\textbf{w}^{T}\textbf{w} + C \label{eq:13}
&&
\end{flalign}
with $C = \frac{1}{2}C_{1} + C_{2} + C_{3}$.
\newline
\newline
Now similar steps are repeated for the factorized component form of the likelihood \eqref{eq:5}. Note that the derivation of the constants, and thus equations, for the \textit{prior} and \textit{evidence} are the same as in respectively \eqref{eq:11} and \eqref{eq:12}.
\begin{flalign}
\ln p(\textbf{w} \mid \mathcal{D}) &= -\frac{1}{2} \sum_{i = 1}^{N} \ln(2\pi) + \ln(d_{i}) + \frac{1}{d_{i}} (\bm{\tau}_{i} - \bm{\phi}_{i}\textbf{w})^{2} \notag \\
&= -\frac{N}{2} \ln(2\pi) - \frac{1}{2} \ln(\text{det}(\textbf{D})) -\frac{1}{2} (\bm{\tau} - \bm{\Phi}\textbf{w})^{T}\textbf{D}^{-1}(\bm{\tau} - \bm{\Phi}\textbf{w}) \notag \\
&= \frac{1}{2} \left(2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \bm{\tau}^{T}\textbf{D}^{-1}\bm{\tau} - N \ln(2\pi) - \ln(\text{det}(\textbf{D})) \right) \notag \\
&= \frac{1}{2} \left(2\bm{\tau}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} - \textbf{w}^{T} \bm{\Phi}^{T} \textbf{D}^{-1} \bm{\Phi} \textbf{w} + C_{1} \right) \label{eq:14}
&&
\end{flalign}
with $C_{1} = -\bm{\tau}^{T}\textbf{D}^{-1}\bm{\tau} - N \ln(2\pi) - \ln(\text{det}(\textbf{D}))$.
\newline
\newline
As we see \eqref{eq:14} is the same \textit{log-likelihood} as in \eqref{eq:10}, and thus the \textit{log-posterior} is of the same form as in \eqref{eq:13}.
\newline
\newline
The hardest part of finding the full posterior distribution is to determine the distribution of the \textit{evidence}. However, finding the MAP is done by maximizing the posterior with respect to \textbf{w}, and thus all the terms that do not depend on \textbf{w} drop out (e.g. the \textit{evidence}). Hence, finding the MAP is simpler.

\bigskip

\textbf{g)} \textit{Solve for $\textbf{w}_{MAP}$ by first taking the derivative of the \underline{log-posterior} with respect to \text{w}, then setting it to 0 and finally solving for \textbf{w}.}
\begin{flalign}
\frac{\partial \ln p(\textbf{w} \mid \mathcal{D})}{\partial \textbf{w}} &= \bm{\Phi}^{T}\textbf{D}^{-1}\bm{\tau} - \bm{\Phi}^{T}\textbf{D}^{-1}\bm{\Phi}\textbf{w} - \alpha^{-1} \textbf{w} = 0 \notag \\
&\Leftrightarrow (\alpha^{-1} \textbf{I} + \bm{\Phi}^{T}\textbf{D}^{-1}\bm{\Phi})\textbf{w} = \bm{\Phi}^{T}\textbf{D}^{-1}\bm{\tau} \notag \\
\implies \textbf{w}_{MAP} &= (\alpha^{-1} \textbf{I} + \bm{\Phi}^{T}\textbf{D}^{-1}\bm{\Phi})^{-1} \bm{\Phi}^{T}\textbf{D}^{-1}\bm{\tau} \label{eq:15}
&&
\end{flalign}

\bigskip

\textbf{h)} \textit{Express the solution for $\textbf{w}_{MAP}$ in terms of the original quantities \textbf{t} and $\bm{\Psi}$ to end up with the final solution stated above.}
\begin{flalign}
\textbf{w}_{MAP} &= (\alpha^{-1} \textbf{I} + (\textbf{A}\bm{\Psi})^{T}\textbf{D}^{-1}\textbf{A}\bm{\Psi})^{-1} (\textbf{A}\bm{\Psi})^{T}\textbf{D}^{-1}\textbf{At} \notag \\
&= (\alpha^{-1} \textbf{I} + \bm{\Psi}^{T}\textbf{A}^{T}\textbf{D}^{-1}\textbf{A}\bm{\Psi})^{-1} \bm{\Psi}^{T}\textbf{A}^{T}\textbf{D}^{-1}\textbf{At} \notag \\
&= (\alpha^{-1} \textbf{I} + \bm{\Psi}^{T}\bm{\Omega}^{-1}\bm{\Psi})^{-1}\bm{\Psi}^{T}\bm{\Omega}^{-1}\textbf{t} \label{eq:16}
&&
\end{flalign}

\bigskip

\section{\underline{ML estimate of angle measurements}}
\textit{Find the maximum likelihood estimate of the angle $\theta$, given that you have two independent noisy measurements, c and s, where c is a measure of the cosine, and s is a measure of the sine, of the angle $\theta$. Assume each measurements has a known Gaussian standard deviation $\sigma$ (same in both cases). To make the solution unique assume $\theta \in [-\pi/2, \pi/2]$. Hint: use that $\sin^{2} \theta + \cos^{2} \theta = 1 \forall \theta \in [0, 2\pi)$ and gather all terms which are independent of $\theta$ into a constant $C$.}

\end{document}
